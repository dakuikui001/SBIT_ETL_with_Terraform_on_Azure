{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37efb2-7ba3-4d2a-9959-e50356a8fcbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./01_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb4e8f09-3118-43d9-9762-e032b8765449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view_name):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74cc5813-7796-45fc-ad72-d77ccf49e02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CDCUpserter:\n",
    "    def __init__(self, merge_query, temp_view_name, id_column, sort_by):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name\n",
    "        self.id_column = id_column\n",
    "        self.sort_by = sort_by\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        window = Window.partitionBy(self.id_column).orderBy(F.col(self.sort_by).desc())\n",
    "\n",
    "        df_micro_batch.filter(F.col(\"update_type\").isin([\"new\", \"update\"])) \\\n",
    "            .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\") \\\n",
    "            .createOrReplaceTempView(self.temp_view_name)\n",
    "        \n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39264564-45fa-412c-a50e-8f49c9e65f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Silver():\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config()\n",
    "        self.checkpoint_base = self.Conf.project_dir + \"checkpoints\"\n",
    "        self.catalog = f\"sbit_{env}_catalog\"\n",
    "        self.db_name = self.Conf.db_name\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "    # --- 1. 用户基本信息 (Idempotent Insert) ---\n",
    "    def upsert_users(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.users a\n",
    "            USING users_delta b\n",
    "            ON a.user_id = b.user_id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"users_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.registered_users_bz\")\n",
    "            .selectExpr(\"user_id\", \"device_id\", \"mac_address\", \"cast(registration_timestamp as timestamp)\")\n",
    "            .withWatermark(\"registration_timestamp\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"device_id\"])\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"users\", \"users_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "\n",
    "    # --- 2. 健身房登录日志 (Conditional Update) ---\n",
    "    def upsert_gym_logs(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.gym_logs a\n",
    "            USING gym_logs_delta b\n",
    "            ON a.mac_address=b.mac_address AND a.gym=b.gym AND a.login=b.login\n",
    "            WHEN MATCHED AND b.logout > a.login AND b.logout > a.logout\n",
    "                THEN UPDATE SET logout = b.logout\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"gym_logs_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.gym_logins_bz\")\n",
    "            .selectExpr(\"mac_address\", \"gym\", \"cast(login as timestamp)\", \"cast(logout as timestamp)\")\n",
    "            .withWatermark(\"login\", \"30 seconds\")\n",
    "            .dropDuplicates([\"mac_address\", \"gym\", \"login\"])\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"gym_logs\", \"gym_logs_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "\n",
    "    # --- 3. 用户配置 (CDC Upsert) ---\n",
    "    def upsert_user_profile(self, once=False, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"user_id string, dob date, sex string, gender string, first_name string, last_name string, address STRUCT<street_address: STRING, city: STRING, state: STRING, zip: INT>, timestamp timestamp, update_type string\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_profile a\n",
    "            USING user_profile_cdc b\n",
    "            ON a.user_id=b.user_id\n",
    "            WHEN MATCHED AND a.updated < b.updated\n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED\n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = CDCUpserter(query, \"user_profile_cdc\", \"user_id\", \"updated\")\n",
    "\n",
    "        df_cdc = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'user_info'\")\n",
    "            .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\")\n",
    "            .select(\"user_id\", F.to_date('dob', 'yyyy-MM-dd').alias('dob'), 'sex', 'gender', 'first_name', 'last_name', 'address.*', F.col('timestamp').cast(\"timestamp\").alias(\"updated\"), \"update_type\")\n",
    "            .withWatermark(\"updated\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"updated\"])\n",
    "        )\n",
    "        return self._write_stream_update(df_cdc, data_upserter, \"user_profile\", \"user_profile_stream\", \"silver_p1\", once, processing_time)\n",
    "\n",
    "    # --- 4. 心率数据 (BPM Idempotent) ---\n",
    "    def upsert_heart_rate(self, once=False, processing_time=\"10 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"device_id String, time TIMESTAMP, heartrate DOUBLE\"\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.heart_rate a\n",
    "            USING heart_rate_delta b\n",
    "            ON a.device_id=b.device_id AND a.time=b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"heart_rate_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'bpm'\")\n",
    "            .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, False).otherwise(True).alias(\"valid\"))\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"device_id\", \"time\"])\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"heart_rate\", \"heart_rate_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "    \n",
    "    # ---5. 锻炼记录 (upsert_workouts) ---\n",
    "    def upsert_workouts(self, once=False, processing_time=\"10 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"user_id string, workout_id string, time timestamp, action string, session_id string\"\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.workouts a\n",
    "            USING workouts_delta b\n",
    "            ON a.user_id=b.user_id AND a.time=b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"workouts_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'workout'\")\n",
    "            .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.user_id\", \"v.workout_id\", \"v.time\", \"v.action\", \"v.session_id\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"time\"])\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"workouts\", \"workouts_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "    \n",
    "    def age_bins(self, dob_col):\n",
    "        from pyspark.sql import functions as F\n",
    "        age_col = F.floor(F.months_between(F.current_date(), dob_col) / 12).alias(\"age\")\n",
    "        return (F.when(age_col < 18, \"under 18\")\n",
    "                .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "                .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "                .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "                .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "                .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "                .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "                .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "                .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "                .when(age_col >= 95, \"95+\")\n",
    "                .otherwise(\"invalid age\").alias(\"age\"))\n",
    "    \n",
    "    def upsert_user_bins(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        # Idempotent - This table is maintained as SCD Type 1 dimension\n",
    "        #            - Insert new user_id records\n",
    "        #            - Update old records using the user_id\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_bins a\n",
    "            USING user_bins_delta b\n",
    "            ON a.user_id = b.user_id\n",
    "            WHEN MATCHED\n",
    "            THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"user_bins_delta\")\n",
    "\n",
    "        df_user = spark.table(f\"{self.catalog}.{self.db_name}.users\").select(\"user_id\")\n",
    "\n",
    "        # Running stream on silver table requires ignoreChanges\n",
    "        # No watermark required - Stream to static join is stateless\n",
    "        df_delta = (spark.readStream\n",
    "                    .option(\"startingVersion\", startingVersion)\n",
    "                    .option(\"ignoreChanges\", True)\n",
    "                    #.option(\"withEventTimeOrder\", \"true\")\n",
    "                    #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                    .table(f\"{self.catalog}.{self.db_name}.user_profile\")\n",
    "                    .join(df_user, [\"user_id\"], \"left\")\n",
    "                    .select(\"user_id\", self.age_bins(F.col(\"dob\")), \"gender\", \"city\", \"state\")\n",
    "                )\n",
    "\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"user_bins\", \"user_bins_upsert_stream\", \"silver_p2\", once, processing_time)\n",
    "    \n",
    "    def upsert_completed_workouts(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        # Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.completed_workouts a\n",
    "            USING completed_workouts_delta b\n",
    "            ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"completed_workouts_delta\")\n",
    "\n",
    "        # 读取开始事件流\n",
    "        df_start = (spark.readStream\n",
    "                    .option(\"startingVersion\", startingVersion)\n",
    "                    .option(\"ignoreDeletes\", True)\n",
    "                    #.option(\"withEventTimeOrder\", \"true\")\n",
    "                    #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                    .table(f\"{self.catalog}.{self.db_name}.workouts\")\n",
    "                    .filter(\"action = 'start'\")\n",
    "                    .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as start_time\")\n",
    "                    .withWatermark(\"start_time\", \"30 seconds\")\n",
    "                    #.dropDuplicates([\"user_id\", \"workout_id\", \"session_id\", \"start_time\"])\n",
    "                )\n",
    "\n",
    "        # 读取停止事件流\n",
    "        df_stop = (spark.readStream\n",
    "                .option(\"startingVersion\", startingVersion)\n",
    "                .option(\"ignoreDeletes\", True)\n",
    "                #.option(\"withEventTimeOrder\", \"true\")\n",
    "                #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                .table(f\"{self.catalog}.{self.db_name}.workouts\")\n",
    "                .filter(\"action = 'stop'\")\n",
    "                .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as end_time\")\n",
    "                .withWatermark(\"end_time\", \"30 seconds\")\n",
    "                #.dropDuplicates([\"user_id\", \"workout_id\", \"session_id\", \"end_time\"])\n",
    "                )\n",
    "\n",
    "        # State cleanup - Define a condition to clean the state\n",
    "        # - stop must occur within 3 hours of start\n",
    "        # - stop < start + 3 hours\n",
    "        join_condition = [\n",
    "            df_start.user_id == df_stop.user_id, \n",
    "            df_start.workout_id == df_stop.workout_id, \n",
    "            df_start.session_id == df_stop.session_id,\n",
    "            df_stop.end_time < df_start.start_time + F.expr('interval 3 hour')\n",
    "        ]\n",
    "\n",
    "        df_delta = (df_start.join(df_stop, join_condition)\n",
    "                    .select(df_start.user_id, df_start.workout_id, df_start.session_id, df_start.start_time, df_stop.end_time)\n",
    "                )\n",
    "\n",
    "        return self._write_stream_append(df_delta, data_upserter, \"completed_workouts\", \"completed_workouts_upsert_stream\", \"silver_p2\", once, processing_time)\n",
    "    \n",
    "    def upsert_workout_bpm(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        # Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.workout_bpm a\n",
    "            USING workout_bpm_delta b\n",
    "            ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id AND a.time=b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter=Upserter(query, \"workout_bpm_delta\")\n",
    "\n",
    "        df_users = spark.read.table(f\"{self.catalog}.{self.db_name}.users\")\n",
    "\n",
    "        # 读取已完成训练的流\n",
    "        df_completed_workouts = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            #.option(\"withEventTimeOrder\", \"true\")\n",
    "            #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.completed_workouts\")\n",
    "            .join(df_users, \"user_id\")\n",
    "            .selectExpr(\"user_id\", \"device_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\")\n",
    "            .withWatermark(\"end_time\", \"30 seconds\")\n",
    "        )\n",
    "\n",
    "        # 读取原始心率流\n",
    "        df_bpm = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            #.option(\"withEventTimeOrder\", \"true\")\n",
    "            #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.heart_rate\")\n",
    "            .filter(\"valid = True\")\n",
    "            .selectExpr(\"device_id\", \"time\", \"heartrate\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "        )\n",
    "\n",
    "        # State cleanup - Define a condition to clean the state\n",
    "        # - Workout could be a maximum of three hours\n",
    "        # - workout must end within 3 hours of bpm\n",
    "        # - workout.end < bpm.time + 3 hours\n",
    "        join_condition = [\n",
    "            df_completed_workouts.device_id == df_bpm.device_id,\n",
    "            df_bpm.time > df_completed_workouts.start_time, \n",
    "            df_bpm.time <= df_completed_workouts.end_time,\n",
    "            df_completed_workouts.end_time < df_bpm.time + F.expr('interval 3 hour')\n",
    "        ]\n",
    "\n",
    "        df_delta = (df_bpm.join(df_completed_workouts, join_condition)\n",
    "                    .select(\"user_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\", \"time\", \"heartrate\")\n",
    "                )\n",
    "        return self._write_stream_append(df_delta, data_upserter, \"workout_bpm\", \"workout_bpm_upsert_stream\", \"silver_p3\", once, processing_time)\n",
    "    \n",
    "\n",
    "    # --- 辅助写入方法 ---\n",
    "    def _write_stream_update(self, df, upserter, path, query_name, pool, once, processing_time):\n",
    "        stream_writer = (df.writeStream\n",
    "            .foreachBatch(upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/{path}\")\n",
    "            .queryName(query_name)\n",
    "        )\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", pool)\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "    \n",
    "    def _write_stream_append(self, df, upserter, path, query_name, pool, once, processing_time):\n",
    "        stream_writer = (df.writeStream\n",
    "            .foreachBatch(upserter.upsert)\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/{path}\")\n",
    "            .queryName(query_name)\n",
    "        )\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", pool)\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "    \n",
    "    def _await_queries(self, once):\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "    \n",
    "    def upsert(self, once=True, processing_time=\"5 seconds\"):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nExecuting silver layer upsert ...\")\n",
    "\n",
    "        # 阶段 1: 基础事实表/维度表同步\n",
    "        self.upsert_users(once, processing_time)\n",
    "        self.upsert_gym_logs(once, processing_time)\n",
    "        self.upsert_user_profile(once, processing_time)\n",
    "        self.upsert_workouts(once, processing_time)\n",
    "        self.upsert_heart_rate(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 1 upsert {int(time.time()) - start} seconds\")\n",
    "\n",
    "        # 阶段 2: 派生维度与训练匹配\n",
    "        self.upsert_user_bins(once, processing_time)\n",
    "        self.upsert_completed_workouts(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 2 upsert {int(time.time()) - start} seconds\")\n",
    "\n",
    "        # 阶段 3: 训练心率明细关联\n",
    "        self.upsert_workout_bpm(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 3 upsert {int(time.time()) - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
